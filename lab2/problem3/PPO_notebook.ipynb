{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70a6b0f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import torch \n",
        "from torch import nn\n",
        "import torch.distributions as dist\n",
        "import random\n",
        "import gymnasium as gym\n",
        "import torch.optim as optim\n",
        "from tqdm import trange\n",
        "from scipy.stats import multivariate_normal\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "18635833",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent(object):\n",
        "    ''' Base agent class\n",
        "\n",
        "        Args:\n",
        "            n_actions (int): actions dimensionality\n",
        "\n",
        "        Attributes:\n",
        "            n_actions (int): where we store the dimensionality of an action\n",
        "    '''\n",
        "    def __init__(self, n_actions: int):\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def forward(self, state: np.ndarray):\n",
        "        ''' Performs a forward computation '''\n",
        "        pass\n",
        "\n",
        "    def backward(self):\n",
        "        ''' Performs a backward pass on the network '''\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
        "    def __init__(self, n_actions: int):\n",
        "        super(RandomAgent, self).__init__(n_actions)\n",
        "\n",
        "    def forward(self, state: np.ndarray) -> np.ndarray:\n",
        "        ''' Compute a random action in [-1, 1]\n",
        "\n",
        "            Returns:\n",
        "                action (np.ndarray): array of float values containing the\n",
        "                    action. The dimensionality is equal to self.n_actions from\n",
        "                    the parent class Agent\n",
        "        '''\n",
        "        return np.clip(-1 + 2 * np.random.rand(self.n_actions), -1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "669668f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(8, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, n_actions: int):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.relu = nn.ReLU()         \n",
        "        self.tanh = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.Ls = nn.Linear(8,400)\n",
        "        self.mu_head1 = nn.Linear(400, 200)\n",
        "        self.mu_head2 = nn.Linear(200, n_actions)\n",
        "        self.sigma_head1 = nn.Linear(400, 200)\n",
        "        self.sigma_head2 = nn.Linear(200, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.Ls(x)\n",
        "        x = self.relu(x)\n",
        "        # mu head\n",
        "        mu = self.mu_head1(x)\n",
        "        mu = self.relu(mu)\n",
        "        mu = self.mu_head2(mu)\n",
        "        mu = self.tanh(mu)\n",
        "        # sigma head\n",
        "        sigma = self.sigma_head1(x)\n",
        "        sigma = self.relu(sigma)\n",
        "        sigma = self.sigma_head2(sigma)\n",
        "        sigma = self.sigmoid(sigma)\n",
        "\n",
        "        return mu, sigma\n",
        " \n",
        "class PPOAgent(Agent):\n",
        "\n",
        "    def __init__(self, n_actions: int):\n",
        "        super(PPOAgent, self).__init__(n_actions)\n",
        "        self.critic_net = CriticNetwork()\n",
        "        self.actor_net = ActorNetwork(n_actions)\n",
        "\n",
        "    def forward(self, state: np.ndarray) -> np.ndarray:\n",
        "        mu, sigma = self.actor_net(torch.tensor(np.expand_dims(state, axis=0)))\n",
        "        a1 = mu[0][0].item() + sigma[0][0].item() * np.random.normal()\n",
        "        a2 = mu[0][1].item() + sigma[0][1].item() * np.random.normal()\n",
        "        return np.clip(np.array([a1, a2], dtype=np.float32), -1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "d5ad1d9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Experience tuple\n",
        "# Experience represents a transition in the environment, including the current state, action taken,\n",
        "# received reward, next state, and whether the episode is done.\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    \"\"\"Replay buffer for storing experiences.\n",
        "    \n",
        "       The experience replay buffer stores past experiences so that the agent can learn from them later.\n",
        "       By sampling randomly from these experiences, the agent avoids overfitting to the most recent \n",
        "       transitions and helps stabilize training.\n",
        "       - The buffer size is limited, and older experiences are discarded to make room for new ones.\n",
        "       - Experiences are stored as tuples of (state, action, reward, next_state, done).\n",
        "       - A batch of experiences is sampled randomly during each training step for updating the Q-values.\"\"\"\n",
        "\n",
        "    def __init__(self, maximum_length):\n",
        "        self.buffer = deque(maxlen=maximum_length)  # Using deque ensures efficient removal of oldest elements\n",
        "\n",
        "    def append(self, experience):\n",
        "        \"\"\"Add a new experience to the buffer\"\"\"\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of the buffer\"\"\"\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample_batch(self, n):\n",
        "        \"\"\"Randomly sample a batch of experiences\"\"\"\n",
        "        if n > len(self.buffer):\n",
        "            raise IndexError('Sample size exceeds buffer size!')\n",
        "        indices = np.random.choice(len(self.buffer), size=n, replace=False)  # Random sampling\n",
        "        batch = [self.buffer[i] for i in indices]  # Create a batch from sampled indices\n",
        "        return zip(*batch)  # Unzip batch into state, action, reward, next_state, and done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6fd2600",
      "metadata": {},
      "outputs": [],
      "source": [
        "N_EPISODES = 250  # Number of training episodes\n",
        "BUFFER_SIZE = 10000  # Size of the replay buffer\n",
        "ACTOR_LEARNING_RATE = 0.001  # Learning rate for the optimizer\n",
        "CRITIC_LEARNING_RATE = 0.001 \n",
        "GAMMA = 0.9\n",
        "M=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "id": "a965cefe",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('LunarLanderContinuous-v3')\n",
        "m = len(env.action_space.high)\n",
        "agent = PPOAgent(m)\n",
        "buffer = ExperienceReplayBuffer(maximum_length=BUFFER_SIZE)\n",
        "actor_optimizer = optim.Adam(agent.actor_net.parameters(), lr=ACTOR_LEARNING_RATE)\n",
        "critic_optimizer = optim.Adam(agent.critic_net.parameters(), lr=CRITIC_LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "id": "a0e5d4b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_action(state, epsilon):\n",
        "    \"\"\"Epsilon-greedy action selection\n",
        "    # We balance exploration and exploitation using epsilon-greedy.\n",
        "    # Exploration: Choose a random action.\n",
        "    # Exploitation: Choose the action with the highest Q-value (the optimal action).\"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # Explore by selecting a random action\n",
        "    else:\n",
        "        state_tensor = torch.tensor([state], dtype=torch.float32)  # Convert state to tensor\n",
        "        return agent.net(state_tensor).argmax().item()  # Exploit by selecting the action with max Q-value\n",
        "\n",
        "# state = env.observation_space.sample()\n",
        "# print(\"Type of sampled state:\", type(state))\n",
        "# print(\"Sampled state shape:\", np.shape(state))\n",
        "\n",
        "#s = torch.tensor(np.expand_dims(state, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c6dad0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode:   8%|â–Š         | 19/250 [00:06<01:13,  3.13it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[139]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m     mu, sigma = agent.actor_net(torch.tensor(np.expand_dims(exp.state, axis=\u001b[32m0\u001b[39m)))\n\u001b[32m     38\u001b[39m     mu, sigma = mu[\u001b[32m0\u001b[39m].detach().numpy(), sigma[\u001b[32m0\u001b[39m].detach().numpy()\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     pi_old_logpdf.append((\u001b[43mmultivariate_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43msigma\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m).logpdf(action))\n\u001b[32m     42\u001b[39m G.reverse()\n\u001b[32m     43\u001b[39m A.reverse()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/EL2805/EL2805_Labs/lab2/problem3/venv/lib/python3.11/site-packages/scipy/stats/_multivariate.py:403\u001b[39m, in \u001b[36mmultivariate_normal_gen.__call__\u001b[39m\u001b[34m(self, mean, cov, allow_singular, seed, **kwds)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, mean=\u001b[38;5;28;01mNone\u001b[39;00m, cov=\u001b[32m1\u001b[39m, allow_singular=\u001b[38;5;28;01mFalse\u001b[39;00m, seed=\u001b[38;5;28;01mNone\u001b[39;00m, **kwds):\n\u001b[32m    399\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a frozen multivariate normal distribution.\u001b[39;00m\n\u001b[32m    400\u001b[39m \n\u001b[32m    401\u001b[39m \u001b[33;03m    See `multivariate_normal_frozen` for more information.\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultivariate_normal_frozen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/EL2805/EL2805_Labs/lab2/problem3/venv/lib/python3.11/site-packages/scipy/stats/_multivariate.py:923\u001b[39m, in \u001b[36mmultivariate_normal_frozen.__init__\u001b[39m\u001b[34m(self, mean, cov, allow_singular, seed, maxpts, abseps, releps)\u001b[39m\n\u001b[32m    880\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a frozen multivariate normal distribution.\u001b[39;00m\n\u001b[32m    881\u001b[39m \n\u001b[32m    882\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    919\u001b[39m \n\u001b[32m    920\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m \u001b[38;5;66;03m# numpy/numpydoc#87  # noqa: E501\u001b[39;00m\n\u001b[32m    921\u001b[39m \u001b[38;5;28mself\u001b[39m._dist = multivariate_normal_gen(seed)\n\u001b[32m    922\u001b[39m \u001b[38;5;28mself\u001b[39m.dim, \u001b[38;5;28mself\u001b[39m.mean, \u001b[38;5;28mself\u001b[39m.cov_object = (\n\u001b[32m--> \u001b[39m\u001b[32m923\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_process_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    924\u001b[39m \u001b[38;5;28mself\u001b[39m.allow_singular = allow_singular \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cov_object._allow_singular\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m maxpts:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/EL2805/EL2805_Labs/lab2/problem3/venv/lib/python3.11/site-packages/scipy/stats/_multivariate.py:427\u001b[39m, in \u001b[36mmultivariate_normal_gen._process_parameters\u001b[39m\u001b[34m(self, mean, cov, allow_singular)\u001b[39m\n\u001b[32m    420\u001b[39m dim, mean, cov = \u001b[38;5;28mself\u001b[39m._process_parameters_psd(\u001b[38;5;28;01mNone\u001b[39;00m, mean, cov)\n\u001b[32m    421\u001b[39m \u001b[38;5;66;03m# After input validation, some methods then processed the arrays\u001b[39;00m\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# with a `_PSD` object and used that to perform computation.\u001b[39;00m\n\u001b[32m    423\u001b[39m \u001b[38;5;66;03m# To avoid branching statements in each method depending on whether\u001b[39;00m\n\u001b[32m    424\u001b[39m \u001b[38;5;66;03m# `cov` is an array or `Covariance` object, we always process the\u001b[39;00m\n\u001b[32m    425\u001b[39m \u001b[38;5;66;03m# array with `_PSD`, and then use wrapper that satisfies the\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;66;03m# `Covariance` interface, `CovViaPSD`.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m psd = \u001b[43m_PSD\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_singular\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    428\u001b[39m cov_object = _covariance.CovViaPSD(psd)\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dim, mean, cov_object\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/EL2805/EL2805_Labs/lab2/problem3/venv/lib/python3.11/site-packages/scipy/stats/_multivariate.py:170\u001b[39m, in \u001b[36m_PSD.__init__\u001b[39m\u001b[34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[39m\n\u001b[32m    165\u001b[39m \u001b[38;5;28mself\u001b[39m._M = np.asarray(M)\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Compute the symmetric eigendecomposition.\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Note that eigh takes care of array conversion, chkfinite,\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;66;03m# and assertion that the matrix is square.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m s, u = \u001b[43mscipy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43meigh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_finite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m eps = _eigvalsh_to_eps(s, cond, rcond)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.min(s) < -eps:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/EL2805/EL2805_Labs/lab2/problem3/venv/lib/python3.11/site-packages/scipy/_lib/_util.py:1220\u001b[39m, in \u001b[36m_apply_over_batch.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1218\u001b[39m batch_shapes = []\n\u001b[32m   1219\u001b[39m core_shapes = []\n\u001b[32m-> \u001b[39m\u001b[32m1220\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, (array, ndim) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndims\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m   1221\u001b[39m     array = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m array \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m np.asarray(array)\n\u001b[32m   1222\u001b[39m     shape = () \u001b[38;5;28;01mif\u001b[39;00m array \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m array.shape\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from audioop import reverse\n",
        "\n",
        "from torch import mul\n",
        "\n",
        "\n",
        "EPISODES = trange(N_EPISODES, desc='Episode: ', leave=True)\n",
        "\n",
        "for i in EPISODES:\n",
        "    # Reset environment data\n",
        "    done, truncated = False, False\n",
        "    state = env.reset()[0]\n",
        "    total_episode_reward = 0.0\n",
        "    t = 0\n",
        "\n",
        "    while not (done or truncated):\n",
        "        action = agent.forward(state)\n",
        "        next_state, reward, terminal, truncated, _ = env.step(action)\n",
        "        done = terminal or truncated\n",
        "        total_episode_reward += reward\n",
        "        buffer.append(Experience(state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "        t += 1\n",
        "\n",
        "    # Batch tensors for the collected trajectory\n",
        "    states = torch.tensor(np.stack([exp.state for exp in buffer.buffer]), dtype=torch.float32)\n",
        "    actions = torch.tensor(np.stack([exp.action for exp in buffer.buffer]), dtype=torch.float32)\n",
        "    rewards = torch.tensor([exp.reward for exp in buffer.buffer], dtype=torch.float32)\n",
        "    dones = torch.tensor([exp.done for exp in buffer.buffer], dtype=torch.float32)\n",
        "\n",
        "    # Discounted returns (vectorized over the episode)\n",
        "    returns = []\n",
        "    running = 0.0\n",
        "    for r, d in zip(reversed(rewards.tolist()), reversed(dones.tolist())):\n",
        "        running = r + GAMMA * running * (1.0 - d)\n",
        "        returns.append(running)\n",
        "    returns = torch.tensor(list(reversed(returns)), dtype=torch.float32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        values = agent.critic_net(states).squeeze(-1)\n",
        "        mu, sigma = agent.actor_net(states)\n",
        "        dist = torch.distributions.Normal(mu, sigma)\n",
        "        pi_old_logpdf = dist.log_prob(actions).sum(dim=1)\n",
        "\n",
        "    advantages = returns - values.detach()\n",
        "\n",
        "    # Placeholder for PPO update step using returns, advantages, and pi_old_logpdf\n",
        "    # for _ in range(M):\n",
        "    #     ... add actor/critic loss and optimizer steps ...\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    #for i in reversed(range(t)):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "505d040b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# env.action_space.sample()\n",
        "# state = env.observation_space.sample()\n",
        "# print(\"Type of sampled state:\", type(state))\n",
        "# print(\"Sampled state shape:\", np.shape(state))\n",
        "\n",
        "# s = torch.tensor(np.expand_dims(state, axis=0))\n",
        "\n",
        "# mu[0][0].item()\n",
        "# mu, sigma = agent.actor_net(s)\n",
        "# agent.forward(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa240a4a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experience(state=0.1, action=0.2, reward=0.3, next_state=0.4, done=0.7)\n",
            "1\n",
            "Experience(state=0.1, action=0.2, reward=0.3, next_state=0.4, done=0.5)\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "a = Experience(0.1, 0.2, 0.3, 0.4, 0.5)\n",
        "b = Experience(0.1, 0.2, 0.3, 0.4, 0.7)\n",
        "\n",
        "for i, exp in zip(reversed(range(len([a, b]))), reversed([a, b])):\n",
        "    print(exp)\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7db8394a",
      "metadata": {},
      "outputs": [],
      "source": [
        "l = [0.1, 0.2, 0.3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad663c65",
      "metadata": {},
      "outputs": [],
      "source": [
        "l.reverse()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ea1ef3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.1, 0.2, 0.3]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f4b49e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of sampled state: <class 'numpy.ndarray'>\n",
            "Sampled state shape: (8,)\n"
          ]
        }
      ],
      "source": [
        "env.action_space.sample()\n",
        "state = env.observation_space.sample()\n",
        "print(\"Type of sampled state:\", type(state))\n",
        "print(\"Sampled state shape:\", np.shape(state))\n",
        "\n",
        "s = torch.tensor(np.expand_dims(state, axis=0))\n",
        "\n",
        "#ActorNetworkmu[0][0].item()\n",
        "mu, sigma = agent.actor_net(s)\n",
        "agent.forward(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b5e952",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-2.7282395362854004"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logpdf.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a2dccb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-99.09631038,  99.60542318])"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f4f63c1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<scipy.stats._multivariate.multivariate_normal_frozen at 0x15d129290>"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multivariate_normal(np.array([100, -100]), np.diag([1,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc88dbc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-1.41893853, -1.41893853])"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multivariate_normal.logpdf([1, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2894691",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.11.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
