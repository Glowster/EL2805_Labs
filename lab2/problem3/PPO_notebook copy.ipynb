{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70a6b0f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import torch \n",
        "from torch import nn\n",
        "import torch.distributions as dist\n",
        "import random\n",
        "import gymnasium as gym\n",
        "import torch.optim as optim\n",
        "from tqdm import trange\n",
        "from scipy.stats import multivariate_normal\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "18635833",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent(object):\n",
        "    ''' Base agent class\n",
        "\n",
        "        Args:\n",
        "            n_actions (int): actions dimensionality\n",
        "\n",
        "        Attributes:\n",
        "            n_actions (int): where we store the dimensionality of an action\n",
        "    '''\n",
        "    def __init__(self, n_actions: int):\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def forward(self, state: np.ndarray):\n",
        "        ''' Performs a forward computation '''\n",
        "        pass\n",
        "\n",
        "    def backward(self):\n",
        "        ''' Performs a backward pass on the network '''\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
        "    def __init__(self, n_actions: int):\n",
        "        super(RandomAgent, self).__init__(n_actions)\n",
        "\n",
        "    def forward(self, state: np.ndarray) -> np.ndarray:\n",
        "        ''' Compute a random action in [-1, 1]\n",
        "\n",
        "            Returns:\n",
        "                action (np.ndarray): array of float values containing the\n",
        "                    action. The dimensionality is equal to self.n_actions from\n",
        "                    the parent class Agent\n",
        "        '''\n",
        "        return np.clip(-1 + 2 * np.random.rand(self.n_actions), -1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "669668f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(8, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, n_actions: int):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.relu = nn.ReLU()         \n",
        "        self.tanh = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.Ls = nn.Linear(8,400)\n",
        "        self.mu_head1 = nn.Linear(400, 200)\n",
        "        self.mu_head2 = nn.Linear(200, n_actions)\n",
        "        self.sigma_head1 = nn.Linear(400, 200)\n",
        "        self.sigma_head2 = nn.Linear(200, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.Ls(x)\n",
        "        x = self.relu(x)\n",
        "        # mu head\n",
        "        mu = self.mu_head1(x)\n",
        "        mu = self.relu(mu)\n",
        "        mu = self.mu_head2(mu)\n",
        "        mu = self.tanh(mu)\n",
        "        # sigma head\n",
        "        sigma = self.sigma_head1(x)\n",
        "        sigma = self.relu(sigma)\n",
        "        sigma = self.sigma_head2(sigma)\n",
        "        sigma = self.sigmoid(sigma)\n",
        "\n",
        "        return mu, sigma\n",
        " \n",
        "class PPOAgent(Agent):\n",
        "\n",
        "    def __init__(self, n_actions: int):\n",
        "        super(PPOAgent, self).__init__(n_actions)\n",
        "        self.critic_net = CriticNetwork()\n",
        "        self.actor_net = ActorNetwork(n_actions)\n",
        "\n",
        "    def forward(self, state: np.ndarray) -> np.ndarray:\n",
        "        mu, sigma = self.actor_net(torch.tensor(np.expand_dims(state, axis=0)))\n",
        "        a1 = mu[0][0].item() + sigma[0][0].item() * np.random.normal()\n",
        "        a2 = mu[0][1].item() + sigma[0][1].item() * np.random.normal()\n",
        "        return np.clip(np.array([a1, a2], dtype=np.float32), -1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d5ad1d9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Experience tuple\n",
        "# Experience represents a transition in the environment, including the current state, action taken,\n",
        "# received reward, next state, and whether the episode is done.\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    \"\"\"Replay buffer for storing experiences.\n",
        "    \n",
        "       The experience replay buffer stores past experiences so that the agent can learn from them later.\n",
        "       By sampling randomly from these experiences, the agent avoids overfitting to the most recent \n",
        "       transitions and helps stabilize training.\n",
        "       - The buffer size is limited, and older experiences are discarded to make room for new ones.\n",
        "       - Experiences are stored as tuples of (state, action, reward, next_state, done).\n",
        "       - A batch of experiences is sampled randomly during each training step for updating the Q-values.\"\"\"\n",
        "\n",
        "    def __init__(self, maximum_length):\n",
        "        self.buffer = deque(maxlen=maximum_length)  # Using deque ensures efficient removal of oldest elements\n",
        "\n",
        "    def append(self, experience):\n",
        "        \"\"\"Add a new experience to the buffer\"\"\"\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of the buffer\"\"\"\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample_batch(self, n):\n",
        "        \"\"\"Randomly sample a batch of experiences\"\"\"\n",
        "        if n > len(self.buffer):\n",
        "            raise IndexError('Sample size exceeds buffer size!')\n",
        "        indices = np.random.choice(len(self.buffer), size=n, replace=False)  # Random sampling\n",
        "        batch = [self.buffer[i] for i in indices]  # Create a batch from sampled indices\n",
        "        return zip(*batch)  # Unzip batch into state, action, reward, next_state, and done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c6fd2600",
      "metadata": {},
      "outputs": [],
      "source": [
        "N_EPISODES = 1600  # Number of training episodes\n",
        "BUFFER_SIZE = 2000  # Size of the replay buffer\n",
        "CRITIC_LEARNING_RATE = 1e-3\n",
        "ACTOR_LEARNING_RATE = 1e-5 # Learning rate for the optimizer\n",
        "GAMMA = 0.99\n",
        "M=10\n",
        "EPSILON=0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a965cefe",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('LunarLanderContinuous-v3')\n",
        "m = len(env.action_space.high)\n",
        "agent = PPOAgent(m)\n",
        "buffer = ExperienceReplayBuffer(maximum_length=BUFFER_SIZE)\n",
        "actor_optimizer = optim.Adam(agent.actor_net.parameters(), lr=ACTOR_LEARNING_RATE)\n",
        "critic_optimizer = optim.Adam(agent.critic_net.parameters(), lr=CRITIC_LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a0e5d4b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_action(state, epsilon):\n",
        "    \"\"\"Epsilon-greedy action selection\n",
        "    # We balance exploration and exploitation using epsilon-greedy.\n",
        "    # Exploration: Choose a random action.\n",
        "    # Exploitation: Choose the action with the highest Q-value (the optimal action).\"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # Explore by selecting a random action\n",
        "    else:\n",
        "        state_tensor = torch.tensor([state], dtype=torch.float32)  # Convert state to tensor\n",
        "        return agent.net(state_tensor).argmax().item()  # Exploit by selecting the action with max Q-value\n",
        "\n",
        "# state = env.observation_space.sample()\n",
        "# print(\"Type of sampled state:\", type(state))\n",
        "# print(\"Sampled state shape:\", np.shape(state))\n",
        "\n",
        "#s = torch.tensor(np.expand_dims(state, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "44c6dad0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode:   0%|          | 1/1600 [00:03<1:25:12,  3.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reward:  -157.43439725390957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode:  25%|██▌       | 401/1600 [01:51<34:35,  1.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reward:  -85.49579295673426\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode:  50%|█████     | 801/1600 [04:15<22:16,  1.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reward:  123.75140524942174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode:  75%|███████▌  | 1201/1600 [05:36<04:52,  1.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reward:  248.13719644076943\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode: 100%|██████████| 1600/1600 [06:28<00:00,  4.12it/s]\n"
          ]
        }
      ],
      "source": [
        "EPISODES = trange(N_EPISODES, desc='Episode: ', leave=True)\n",
        "\n",
        "for i in EPISODES:\n",
        "\n",
        "    buffer = ExperienceReplayBuffer(maximum_length=BUFFER_SIZE)\n",
        "\n",
        "     # Reset enviroment data\n",
        "    done, truncated = False, False\n",
        "    state = env.reset()[0]\n",
        "    total_episode_reward = 0.\n",
        "    t = 0\n",
        "    rewards = []\n",
        "\n",
        "    while not (done or truncated):\n",
        "        action = agent.forward(state)\n",
        "        next_state, reward, terminal, truncated, _ = env.step(action)\n",
        "        done = terminal or truncated \n",
        "        total_episode_reward += reward\n",
        "        buffer.append(Experience(state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "        t+= 1\n",
        "    \n",
        "    G = []\n",
        "    A = []\n",
        "    pi_old_logpdf = []\n",
        "\n",
        "    running = 0\n",
        "\n",
        "\n",
        "    for exp in reversed(buffer.buffer):\n",
        "        running = GAMMA * running + exp.reward\n",
        "        G.append(running)\n",
        "        A.append(running - agent.critic_net(torch.tensor(np.expand_dims(exp.state, axis=0))).item())\n",
        "\n",
        "        mu, sigma = agent.actor_net(torch.tensor(np.expand_dims(exp.state, axis=0)))\n",
        "        mu, sigma = mu[0].detach(), sigma[0].detach()\n",
        "\n",
        "        #pi_old_logpdf.append(torch.distributions.MultivariateNormal(mu, torch.diag(sigma**2)).log_prob(torch.tensor(exp.action)))\n",
        "        \n",
        "        #pi_old_logpdf.append((multivariate_normal(mu, np.diag(sigma**2))).logpdf(exp.action))\n",
        "\n",
        "        \n",
        "    \n",
        "    G.reverse()\n",
        "    A.reverse()\n",
        "    G = torch.tensor(G)\n",
        "    A = torch.tensor(A)\n",
        "    \n",
        "    # s = torch.tensor([[exp.state] for exp in buffer.buffer]) \n",
        "    # a = [exp.action for exp in buffer.buffer]\n",
        "    states = torch.as_tensor(\n",
        "    np.stack([exp.state for exp in buffer.buffer]), \n",
        "    dtype=torch.float32, \n",
        "    )\n",
        "    actions = torch.as_tensor(\n",
        "        np.stack([exp.action for exp in buffer.buffer]), \n",
        "        dtype=torch.float32, \n",
        "    )\n",
        "\n",
        "\n",
        "    with torch.no_grad():  # old policy should not require gradients\n",
        "        mu_old, sigma_old = agent.actor_net(states)   # shapes [T, act_dim]\n",
        "\n",
        "        dist_old = torch.distributions.MultivariateNormal(\n",
        "            loc=mu_old,\n",
        "            covariance_matrix=torch.diag_embed(sigma_old ** 2)  # [T, act_dim, act_dim]\n",
        "        )\n",
        "\n",
        "        pi_old_logpdf = dist_old.log_prob(actions)    # shape [T]\n",
        "\n",
        "    actor_loss = 0\n",
        "    critic_loss = 0\n",
        "    for m in range(M):\n",
        "        # critic\n",
        "        values = agent.critic_net(states).squeeze(-1)\n",
        "        critic_loss = torch.mean((values - G) ** 2)\n",
        "\n",
        "        # actor\n",
        "        mu, sigma = agent.actor_net(states)\n",
        "        dist = torch.distributions.MultivariateNormal(\n",
        "            mu, torch.diag_embed(sigma ** 2)\n",
        "        )\n",
        "        log_probs = dist.log_prob(actions)\n",
        "\n",
        "        r = (log_probs - pi_old_logpdf).exp()\n",
        "        clipped_r = torch.clamp(r, 1.0 - EPSILON, 1.0 + EPSILON)\n",
        "\n",
        "        surr1 = r * A\n",
        "        surr2 = clipped_r * A\n",
        "        actor_loss = -torch.mean(torch.min(surr1, surr2))\n",
        "\n",
        "        critic_optimizer.zero_grad()\n",
        "        actor_optimizer.zero_grad()\n",
        "\n",
        "        critic_loss.backward()\n",
        "        actor_loss.backward()\n",
        "\n",
        "        critic_optimizer.step()\n",
        "        actor_optimizer.step()\n",
        "        \n",
        "\n",
        "    if i % 400 == 0:\n",
        "        eval_r = []\n",
        "        for i in range(50):\n",
        "            state = env.reset()[0]\n",
        "            done, truncated = False, False\n",
        "\n",
        "            total_episode_reward = 0.\n",
        "\n",
        "            while not (done or truncated):\n",
        "                action = agent.forward(state)\n",
        "                next_state, reward, done, truncated, _ = env.step(action)\n",
        "                total_episode_reward += reward\n",
        "                state = next_state\n",
        "\n",
        "            eval_r.append(total_episode_reward)\n",
        "\n",
        "        print(\"reward: \", np.mean(eval_r))\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "505d040b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# env.action_space.sample()\n",
        "# state = env.observation_space.sample()\n",
        "# print(\"Type of sampled state:\", type(state))\n",
        "# print(\"Sampled state shape:\", np.shape(state))\n",
        "\n",
        "# s = torch.tensor(np.expand_dims(state, axis=0))\n",
        "\n",
        "# mu[0][0].item()\n",
        "# mu, sigma = agent.actor_net(s)\n",
        "# agent.forward(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa240a4a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experience(state=0.1, action=0.2, reward=0.3, next_state=0.4, done=0.7)\n",
            "1\n",
            "Experience(state=0.1, action=0.2, reward=0.3, next_state=0.4, done=0.5)\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "a = Experience(0.1, 0.2, 0.3, 0.4, 0.5)\n",
        "b = Experience(0.1, 0.2, 0.3, 0.4, 0.7)\n",
        "\n",
        "for i, exp in zip(reversed(range(len([a, b]))), reversed([a, b])):\n",
        "    print(exp)\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7db8394a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the trained actor (agent) network\n",
        "torch.save(agent.actor_net, 'neural-network-3-actor.pth')\n",
        "\n",
        "# Save the trained critic network\n",
        "torch.save(agent.critic_net, 'neural-network-3-critic.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad663c65",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull out the weights from the actor and critic networks\n",
        "\n",
        "# For torch nn.Module, use .state_dict() or .parameters()\n",
        "actor_weights = agent.actor_net.state_dict()\n",
        "critic_weights = agent.critic_net.state_dict()\n",
        "\n",
        "print(\"Actor network weights:\")\n",
        "for name, param in actor_weights.items():\n",
        "    print(name, param.shape)\n",
        "\n",
        "print(\"\\nCritic network weights:\")\n",
        "for name, param in critic_weights.items():\n",
        "    print(name, param.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ea1ef3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.1, 0.2, 0.3]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f4b49e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of sampled state: <class 'numpy.ndarray'>\n",
            "Sampled state shape: (8,)\n"
          ]
        }
      ],
      "source": [
        "env.action_space.sample()\n",
        "state = env.observation_space.sample()\n",
        "print(\"Type of sampled state:\", type(state))\n",
        "print(\"Sampled state shape:\", np.shape(state))\n",
        "\n",
        "s = torch.tensor(np.expand_dims(state, axis=0))\n",
        "\n",
        "#ActorNetworkmu[0][0].item()\n",
        "mu, sigma = agent.actor_net(s)\n",
        "agent.forward(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b5e952",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-2.7282395362854004"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logpdf.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a2dccb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-99.09631038,  99.60542318])"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f4f63c1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<scipy.stats._multivariate.multivariate_normal_frozen at 0x15d129290>"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multivariate_normal(np.array([100, -100]), np.diag([1,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc88dbc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-1.41893853, -1.41893853])"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multivariate_normal.logpdf([1, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2894691",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActorNetworkVar(nn.Module):\n",
        "    def __init__(self, n_actions: int):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.relu = nn.ReLU()         \n",
        "        self.tanh = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.Ls = nn.Linear(8,400)\n",
        "        self.mu_head1 = nn.Linear(400, 200)\n",
        "        self.mu_head2 = nn.Linear(200, n_actions)\n",
        "        self.sigma_head1 = nn.Linear(400, 200)\n",
        "        self.sigma_head2 = nn.Linear(200, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        if x.ndim == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.Ls(x)\n",
        "        x = self.relu(x)\n",
        "        # mu head\n",
        "        mu = self.mu_head1(x)\n",
        "        mu = self.relu(mu)\n",
        "        mu = self.mu_head2(mu)\n",
        "        mu = self.tanh(mu)\n",
        "        # sigma head\n",
        "        sigma = self.sigma_head1(x)\n",
        "        sigma = self.relu(sigma)\n",
        "        sigma = self.sigma_head2(sigma)\n",
        "        sigma = self.sigmoid(sigma)\n",
        "\n",
        "        return mu, sigma**2\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "b8fd28f2",
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "ActorNetworkVar.__init__() missing 1 required positional argument: 'n_actions'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m actor_net_var = \u001b[43mActorNetworkVar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mTypeError\u001b[39m: ActorNetworkVar.__init__() missing 1 required positional argument: 'n_actions'"
          ]
        }
      ],
      "source": [
        "actor_net_var = ActorNetworkVar()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.11.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
