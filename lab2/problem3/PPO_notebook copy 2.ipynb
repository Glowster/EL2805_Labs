{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70a6b0f9",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "import torch \n",
        "from torch import nn\n",
        "import torch.distributions as dist\n",
        "import random\n",
        "import gymnasium as gym\n",
        "import torch.optim as optim\n",
        "from tqdm import trange\n",
        "from scipy.stats import multivariate_normal\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "18635833",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Agent(object):\n",
        "    ''' Base agent class\n",
        "\n",
        "        Args:\n",
        "            n_actions (int): actions dimensionality\n",
        "\n",
        "        Attributes:\n",
        "            n_actions (int): where we store the dimensionality of an action\n",
        "    '''\n",
        "    def __init__(self, n_actions: int):\n",
        "        self.n_actions = n_actions\n",
        "\n",
        "    def forward(self, state: np.ndarray):\n",
        "        ''' Performs a forward computation '''\n",
        "        pass\n",
        "\n",
        "    def backward(self):\n",
        "        ''' Performs a backward pass on the network '''\n",
        "        pass\n",
        "\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "    ''' Agent taking actions uniformly at random, child of the class Agent'''\n",
        "    def __init__(self, n_actions: int):\n",
        "        super(RandomAgent, self).__init__(n_actions)\n",
        "\n",
        "    def forward(self, state: np.ndarray) -> np.ndarray:\n",
        "        ''' Compute a random action in [-1, 1]\n",
        "\n",
        "            Returns:\n",
        "                action (np.ndarray): array of float values containing the\n",
        "                    action. The dimensionality is equal to self.n_actions from\n",
        "                    the parent class Agent\n",
        "        '''\n",
        "        return np.clip(-1 + 2 * np.random.rand(self.n_actions), -1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "669668f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(8, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, n_actions: int):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.relu = nn.ReLU()         \n",
        "        self.tanh = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.Ls = nn.Linear(8,400)\n",
        "        self.mu_head1 = nn.Linear(400, 200)\n",
        "        self.mu_head2 = nn.Linear(200, n_actions)\n",
        "        self.sigma_head1 = nn.Linear(400, 200)\n",
        "        self.sigma_head2 = nn.Linear(200, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.Ls(x)\n",
        "        x = self.relu(x)\n",
        "        # mu head\n",
        "        mu = self.mu_head1(x)\n",
        "        mu = self.relu(mu)\n",
        "        mu = self.mu_head2(mu)\n",
        "        mu = self.tanh(mu)\n",
        "        # sigma head\n",
        "        sigma = self.sigma_head1(x)\n",
        "        sigma = self.relu(sigma)\n",
        "        sigma = self.sigma_head2(sigma)\n",
        "        sigma = self.sigmoid(sigma)\n",
        "\n",
        "        return mu, sigma\n",
        " \n",
        "class PPOAgent(Agent):\n",
        "\n",
        "    def __init__(self, n_actions: int):\n",
        "        super(PPOAgent, self).__init__(n_actions)\n",
        "        self.critic_net = CriticNetwork()\n",
        "        self.actor_net = ActorNetwork(n_actions)\n",
        "\n",
        "    def forward(self, state: np.ndarray) -> np.ndarray:\n",
        "        mu, sigma = self.actor_net(torch.tensor(np.expand_dims(state, axis=0)))\n",
        "        a1 = mu[0][0].item() + sigma[0][0].item() * np.random.normal()\n",
        "        a2 = mu[0][1].item() + sigma[0][1].item() * np.random.normal()\n",
        "        return np.clip(np.array([a1, a2], dtype=np.float32), -1, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d5ad1d9a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Experience tuple\n",
        "# Experience represents a transition in the environment, including the current state, action taken,\n",
        "# received reward, next state, and whether the episode is done.\n",
        "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    \"\"\"Replay buffer for storing experiences.\n",
        "    \n",
        "       The experience replay buffer stores past experiences so that the agent can learn from them later.\n",
        "       By sampling randomly from these experiences, the agent avoids overfitting to the most recent \n",
        "       transitions and helps stabilize training.\n",
        "       - The buffer size is limited, and older experiences are discarded to make room for new ones.\n",
        "       - Experiences are stored as tuples of (state, action, reward, next_state, done).\n",
        "       - A batch of experiences is sampled randomly during each training step for updating the Q-values.\"\"\"\n",
        "\n",
        "    def __init__(self, maximum_length):\n",
        "        self.buffer = deque(maxlen=maximum_length)  # Using deque ensures efficient removal of oldest elements\n",
        "\n",
        "    def append(self, experience):\n",
        "        \"\"\"Add a new experience to the buffer\"\"\"\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of the buffer\"\"\"\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample_batch(self, n):\n",
        "        \"\"\"Randomly sample a batch of experiences\"\"\"\n",
        "        if n > len(self.buffer):\n",
        "            raise IndexError('Sample size exceeds buffer size!')\n",
        "        indices = np.random.choice(len(self.buffer), size=n, replace=False)  # Random sampling\n",
        "        batch = [self.buffer[i] for i in indices]  # Create a batch from sampled indices\n",
        "        return zip(*batch)  # Unzip batch into state, action, reward, next_state, and done\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c6fd2600",
      "metadata": {},
      "outputs": [],
      "source": [
        "N_EPISODES = 1600  # Number of training episodes\n",
        "BUFFER_SIZE = 2000  # Size of the replay buffer\n",
        "CRITIC_LEARNING_RATE = 1e-3\n",
        "ACTOR_LEARNING_RATE = 1e-5 # Learning rate for the optimizer\n",
        "GAMMA = 0.99\n",
        "M=10\n",
        "EPSILON=0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a965cefe",
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('LunarLanderContinuous-v3')\n",
        "m = len(env.action_space.high)\n",
        "agent = PPOAgent(m)\n",
        "buffer = ExperienceReplayBuffer(maximum_length=BUFFER_SIZE)\n",
        "actor_optimizer = optim.Adam(agent.actor_net.parameters(), lr=ACTOR_LEARNING_RATE)\n",
        "critic_optimizer = optim.Adam(agent.critic_net.parameters(), lr=CRITIC_LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a0e5d4b9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_action(state, epsilon):\n",
        "    \"\"\"Epsilon-greedy action selection\n",
        "    # We balance exploration and exploitation using epsilon-greedy.\n",
        "    # Exploration: Choose a random action.\n",
        "    # Exploitation: Choose the action with the highest Q-value (the optimal action).\"\"\"\n",
        "    if random.random() < epsilon:\n",
        "        return env.action_space.sample()  # Explore by selecting a random action\n",
        "    else:\n",
        "        state_tensor = torch.tensor([state], dtype=torch.float32)  # Convert state to tensor\n",
        "        return agent.net(state_tensor).argmax().item()  # Exploit by selecting the action with max Q-value\n",
        "\n",
        "# state = env.observation_space.sample()\n",
        "# print(\"Type of sampled state:\", type(state))\n",
        "# print(\"Sampled state shape:\", np.shape(state))\n",
        "\n",
        "#s = torch.tensor(np.expand_dims(state, axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44c6dad0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode:   0%|          | 0/1600 [00:00<?, ?it/s]/var/folders/z4/z52znr751d7gzxk5s8qjss140000gn/T/ipykernel_78257/3021086238.py:51: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:256.)\n",
            "  s = torch.tensor([[exp.state] for exp in buffer.buffer])\n",
            "Episode:   0%|          | 1/1600 [00:02<1:05:40,  2.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reward:  -325.94727966625794\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Episode:   0%|          | 2/1600 [00:04<1:05:49,  2.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reward:  -295.1034739016345\n"
          ]
        }
      ],
      "source": [
        "EPISODES = trange(N_EPISODES, desc='Episode: ', leave=True)\n",
        "\n",
        "for i in EPISODES:\n",
        "\n",
        "    buffer = ExperienceReplayBuffer(maximum_length=BUFFER_SIZE)\n",
        "\n",
        "     # Reset enviroment data\n",
        "    done, truncated = False, False\n",
        "    state = env.reset()[0]\n",
        "    total_episode_reward = 0.\n",
        "    t = 0\n",
        "    rewards = []\n",
        "\n",
        "    while not (done or truncated):\n",
        "        action = agent.forward(state)\n",
        "        next_state, reward, terminal, truncated, _ = env.step(action)\n",
        "        done = terminal or truncated \n",
        "        total_episode_reward += reward\n",
        "        buffer.append(Experience(state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "        t+= 1\n",
        "    \n",
        "    G = []\n",
        "    A = []\n",
        "    pi_old_logpdf = []\n",
        "\n",
        "    running = 0\n",
        "\n",
        "\n",
        "    for exp in reversed(buffer.buffer):\n",
        "        running = GAMMA * running + exp.reward\n",
        "        G.append(running)\n",
        "        A.append(running - agent.critic_net(torch.tensor(np.expand_dims(exp.state, axis=0))).item())\n",
        "\n",
        "        mu, sigma = agent.actor_net(torch.tensor(np.expand_dims(exp.state, axis=0)))\n",
        "        mu, sigma = mu[0].detach(), sigma[0].detach()\n",
        "\n",
        "        pi_old_logpdf.append(torch.distributions.MultivariateNormal(mu, torch.diag(sigma**2)).log_prob(torch.tensor(exp.action)))\n",
        "        \n",
        "        #pi_old_logpdf.append((multivariate_normal(mu, np.diag(sigma**2))).logpdf(exp.action))\n",
        "\n",
        "        \n",
        "    \n",
        "    G.reverse()\n",
        "    A.reverse()\n",
        "    pi_old_logpdf.reverse()\n",
        "    G = torch.tensor(G)\n",
        "    A = torch.tensor(A)\n",
        "    pi_old_logpdf = torch.tensor(pi_old_logpdf)\n",
        "    \n",
        "    s = torch.tensor([[exp.state] for exp in buffer.buffer]) \n",
        "    a = [exp.action for exp in buffer.buffer]\n",
        "\n",
        "    actor_loss = 0\n",
        "    critic_loss = 0\n",
        "    for m in range(M):\n",
        "\n",
        "\n",
        "\n",
        "        critic_loss = torch.mean(torch.square(torch.sub(torch.squeeze(agent.critic_net(s)), G)))\n",
        "\n",
        "        mu_sigmas = [agent.actor_net(torch.tensor(np.expand_dims(exp.state, axis=0))) for exp in buffer.buffer]\n",
        "\n",
        "        pi_theta_pdflog = torch.stack([\n",
        "            torch.distributions.MultivariateNormal(\n",
        "                mu_sigma[0][0], torch.diag(mu_sigma[1][0]**2)\n",
        "            ).log_prob(torch.tensor(a[k], dtype=mu_sigma[0].dtype, device=mu_sigma[0].device))\n",
        "            for k, mu_sigma in enumerate(mu_sigmas)\n",
        "        ])\n",
        "\n",
        "        r = torch.sub(pi_theta_pdflog, pi_old_logpdf).exp()\n",
        "        c_eps = torch.max(torch.ones_like(r) * (1 - EPSILON), torch.min(r, torch.ones_like(r)*(1+EPSILON)))\n",
        "\n",
        "        actor_loss = -torch.mean(torch.min(torch.mul(r,A), torch.mul(c_eps, A)))\n",
        "\n",
        "        critic_optimizer.zero_grad()\n",
        "        actor_optimizer.zero_grad()\n",
        "\n",
        "        critic_loss.backward()\n",
        "        actor_loss.backward()\n",
        "\n",
        "        critic_optimizer.step()\n",
        "        actor_optimizer.step()\n",
        "\n",
        "    eval_r = []\n",
        "    for i in range(50):\n",
        "        state = env.reset()[0]\n",
        "        done, truncated = False, False\n",
        "\n",
        "        total_episode_reward = 0.\n",
        "\n",
        "        while not (done or truncated):\n",
        "            action = agent.forward(state)\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "            total_episode_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        eval_r.append(total_episode_reward)\n",
        "\n",
        "    print(\"reward: \", np.mean(eval_r))\n",
        "        \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "505d040b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# env.action_space.sample()\n",
        "# state = env.observation_space.sample()\n",
        "# print(\"Type of sampled state:\", type(state))\n",
        "# print(\"Sampled state shape:\", np.shape(state))\n",
        "\n",
        "# s = torch.tensor(np.expand_dims(state, axis=0))\n",
        "\n",
        "# mu[0][0].item()\n",
        "# mu, sigma = agent.actor_net(s)\n",
        "# agent.forward(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa240a4a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experience(state=0.1, action=0.2, reward=0.3, next_state=0.4, done=0.7)\n",
            "1\n",
            "Experience(state=0.1, action=0.2, reward=0.3, next_state=0.4, done=0.5)\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "a = Experience(0.1, 0.2, 0.3, 0.4, 0.5)\n",
        "b = Experience(0.1, 0.2, 0.3, 0.4, 0.7)\n",
        "\n",
        "for i, exp in zip(reversed(range(len([a, b]))), reversed([a, b])):\n",
        "    print(exp)\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7db8394a",
      "metadata": {},
      "outputs": [],
      "source": [
        "l = [0.1, 0.2, 0.3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad663c65",
      "metadata": {},
      "outputs": [],
      "source": [
        "l.reverse()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34ea1ef3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.1, 0.2, 0.3]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7f4b49e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type of sampled state: <class 'numpy.ndarray'>\n",
            "Sampled state shape: (8,)\n"
          ]
        }
      ],
      "source": [
        "env.action_space.sample()\n",
        "state = env.observation_space.sample()\n",
        "print(\"Type of sampled state:\", type(state))\n",
        "print(\"Sampled state shape:\", np.shape(state))\n",
        "\n",
        "s = torch.tensor(np.expand_dims(state, axis=0))\n",
        "\n",
        "#ActorNetworkmu[0][0].item()\n",
        "mu, sigma = agent.actor_net(s)\n",
        "agent.forward(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b5e952",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-2.7282395362854004"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logpdf.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81a2dccb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-99.09631038,  99.60542318])"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f4f63c1",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<scipy.stats._multivariate.multivariate_normal_frozen at 0x15d129290>"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multivariate_normal(np.array([100, -100]), np.diag([1,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbc88dbc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-1.41893853, -1.41893853])"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multivariate_normal.logpdf([1, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2894691",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv (3.11.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
